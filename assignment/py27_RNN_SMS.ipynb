{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Collecting torchtext==0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/80/1cde2a940fe42d5572487e47533f4b08302a0dd2c64bbd04116731cd7109/torchtext-0.4.0.tar.gz (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 15.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from torchtext==0.4.0) (4.36.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from torchtext==0.4.0) (2.22.0)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from torchtext==0.4.0) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from torchtext==0.4.0) (1.16.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from torchtext==0.4.0) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from requests->torchtext==0.4.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from requests->torchtext==0.4.0) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages (from requests->torchtext==0.4.0) (2019.11.28)\n",
      "Building wheels for collected packages: torchtext\n",
      "  Building wheel for torchtext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchtext: filename=torchtext-0.4.0-cp27-none-any.whl size=52131 sha256=4ea0aecfa9bf43f28bf88597903bc5e7459546a16855b5f4cd575ec214401144\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/7f/0b/a7/53f554f01d205ac7039ef96028eb886f52e235cdfae5ecf7ef\n",
      "Successfully built torchtext\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 하이퍼파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "word_vec_size = 256\n",
    "dropout_p = 0.3\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 4\n",
    "\n",
    "## yhk 추가\n",
    "learning_rate = 0.001 # default is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SMS train, test dataset 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = DataLoader(\n",
    "    train_fn = './sms.maxlen.uniq.shuf.train.tsv',\n",
    "    batch_size = batch_size,\n",
    "    valid_ratio = .2,\n",
    "    device = -1,\n",
    "    max_vocab = 999999,\n",
    "    min_freq = 5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = DataLoader(\n",
    "    train_fn = './sms.maxlen.uniq.shuf.test.tsv',\n",
    "    batch_size = batch_size,\n",
    "    valid_ratio = .01, # 모두 train\n",
    "    device = -1,\n",
    "    max_vocab = 999999,\n",
    "    min_freq = 5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 대략적인 데이터 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('|train| =', 3722, '|valid| =', 930)\n",
      "('|vocab| =', 1547, '|classes| =', 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"|train| =\", len(loaders.train_loader.dataset),\n",
    "       '|valid| =', len(loaders.valid_loader.dataset))\n",
    "\n",
    "vocab_size = len(loaders.text.vocab)\n",
    "num_classes = len(loaders.label.vocab)\n",
    "print('|vocab| =', vocab_size, '|classes| =', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 로드 함수\n",
    "학습시킬 때 batch_size 단위로 끊어서 로드하기 위함\n",
    "\n",
    "## 데이터 로드함수 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "한 번에 로드되는 데이터 크기\n",
      "128\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(7,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(7,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(7,)\n",
      "[1]\n",
      "한 번에 로드되는 데이터 크기\n",
      "128\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(19,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(19,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(19,)\n",
      "[2]\n",
      "한 번에 로드되는 데이터 크기\n",
      "128\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(16,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(16,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(16,)\n",
      "[3]\n",
      "한 번에 로드되는 데이터 크기\n",
      "128\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(34,)\n",
      "label: \n",
      "0\n",
      "text: \n",
      "(34,)\n",
      "label: \n",
      "1\n",
      "text: \n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "n = 3 # 샘플로 출력할 데이터 개수\n",
    "for i, data in enumerate(loaders.train_loader):\n",
    "    labels = data.label\n",
    "    texts = data.text\n",
    "    \n",
    "    if i > n:\n",
    "        break\n",
    "    print(\"[%d]\"%i)\n",
    "    print(\"한 번에 로드되는 데이터 크기\")\n",
    "    print(len(labels))\n",
    "    \n",
    "    # print\n",
    "    for j in range(n):\n",
    "        label = labels[j].numpy() # tensor -> numpy 로 변환\n",
    "        text = texts[j].numpy()\n",
    "        print(\"label: \")\n",
    "        print(label)\n",
    "        print(\"text: \")\n",
    "        print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network(many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_size,\n",
    "                word_vec_size,\n",
    "                hidden_size,\n",
    "                n_classes,\n",
    "                num_layers=4,\n",
    "                dropout_p=0.3):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        #입력 차원, 출력 차원\n",
    "        self.emb = nn.Embedding(input_size, word_vec_size)\n",
    "        self.lstm = nn.LSTM(input_size = word_vec_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout_p,\n",
    "                            batch_first = True,\n",
    "                            bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.activation = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        # 시험문제 빈칸 나올수도..?\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)    \n",
    "        x,_ = self.lstm(x)\n",
    "        out = self.activation(self.fc(x[:,-1]))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=vocab_size,\n",
    "           word_vec_size=word_vec_size,\n",
    "           hidden_size = hidden_size,\n",
    "           n_classes = num_classes,\n",
    "           num_layers = num_layers,\n",
    "           dropout_p = dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccr(dloader, imodel):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval() # test mode\n",
    "    for i, data in enumerate(dloader):\n",
    "        texts = data.text.to(device)\n",
    "        labels = data.label.to(device)\n",
    "        \n",
    "        #Forward prop\n",
    "        output = model(texts)\n",
    "        _, output_index = torch.max(output, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (output_index == labels).sum().float()\n",
    "        \n",
    "        model.train()\n",
    "        return (100*correct/total).numpy() # tensor -> numpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 10.16\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Data : %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. loss, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [1/10], Step [10/30], Loss:  0.3428, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [1/10], Step [20/30], Loss:  0.3182, Accr : 96.88\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [1/10], Step [30/30], Loss:  0.2008, Accr : 96.88\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [2/10], Step [10/30], Loss:  0.0522, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [2/10], Step [20/30], Loss:  0.9393, Accr : 96.88\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [2/10], Step [30/30], Loss:  0.4107, Accr : 96.88\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [3/10], Step [10/30], Loss:  0.1270, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [3/10], Step [20/30], Loss:  0.6070, Accr : 78.12\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [3/10], Step [30/30], Loss:  0.4024, Accr : 96.88\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [4/10], Step [10/30], Loss:  0.6623, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [4/10], Step [20/30], Loss:  0.8386, Accr : 96.88\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [4/10], Step [30/30], Loss:  0.1475, Accr : 96.09\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [5/10], Step [10/30], Loss:  0.0603, Accr : 96.09\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [5/10], Step [20/30], Loss:  0.2172, Accr : 96.09\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [5/10], Step [30/30], Loss:  0.0403, Accr : 96.09\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [6/10], Step [10/30], Loss:  0.0616, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [6/10], Step [20/30], Loss:  0.0083, Accr : 93.75\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [6/10], Step [30/30], Loss:  0.0442, Accr : 96.88\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [7/10], Step [10/30], Loss:  0.1263, Accr : 96.09\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [7/10], Step [20/30], Loss:  0.2003, Accr : 96.88\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [7/10], Step [30/30], Loss:  0.0860, Accr : 96.88\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [8/10], Step [10/30], Loss:  0.0097, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [8/10], Step [20/30], Loss:  0.0661, Accr : 96.09\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [8/10], Step [30/30], Loss:  0.0640, Accr : 95.31\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [9/10], Step [10/30], Loss:  0.0637, Accr : 94.53\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [9/10], Step [20/30], Loss:  0.0607, Accr : 96.09\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [9/10], Step [30/30], Loss:  0.0365, Accr : 96.09\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [10/10], Step [10/30], Loss:  0.0417, Accr : 96.88\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [10/10], Step [20/30], Loss:  0.0194, Accr : 96.88\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [10/10], Step [30/30], Loss:  0.0368, Accr : 96.88\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(loaders.train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loaders.train_loader):\n",
    "        texts = data.text.to(device)\n",
    "        labels = data.label.to(device)\n",
    "        \n",
    "        print(\"[%d]\" %i)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1)  % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accr : {:.2f}'.\n",
    "                 format(epoch+1, num_epochs, i+1, total_step, loss.item(), ComputeAccr(loaders.valid_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 96.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Data : %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 학습된 파라미터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "netname = './nets/rnn_weight.pkl'\n",
    "torch.save(model, netname, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 학습된 파라미터 로드\n",
    "실무에서 학습된 파라미터 로드하고 싶다면 : 5,6,8 과정 생략한 채 실행\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netname = './nets/rnn_weight.pkl'\n",
    "model = torch.load(netname)\n",
    "print(\"Accuracy of Test Data : %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
